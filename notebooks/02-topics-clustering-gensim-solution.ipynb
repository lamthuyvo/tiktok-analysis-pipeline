{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae37f5d1-e38d-409f-9ab6-b54556d79eb9",
   "metadata": {},
   "source": [
    "### Text cleaning and topic modeling\n",
    "\n",
    "This notebook is an example of topic modeling adapted from [this writeup](https://medium.com/@sayahfares19/text-analysis-topic-modelling-with-spacy-gensim-4cd92ef06e06).\n",
    "\n",
    "It performs the following tasks:\n",
    "\n",
    "- the first part of the notebook loads texts from a spreadsheet and turns them into one large corpuse\n",
    "- then we walk through various ways in which we can analyze and clean our corpus using spaCy (this includes taking out `stopwords` — words most often used in the English language and lemmatizing our corpus)\n",
    "- to better understand how a model works this notebook also explores some funcationalities of spaCy\n",
    "- the last parts of this notebook then make a simple topics model from the cleaned language data\n",
    "\n",
    "The libraries we will use are:\n",
    "- `pandas`: for reading in and exporting spreadsheets\n",
    "- `spacy`: a natural language processing library that contains various models trained on various languages\n",
    "- `gensim`: a library for topic modelling, document indexing and similarity retrieval with large corpora. In this case we will use it for topic modeling, the process of clustering words that seem to be used a lot in relation to one another. The algorithms built into genim that this notebook uses are called [Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2) and [Latent Semantic Analysis (LSA)](https://blog.marketmuse.com/glossary/latent-semantic-analysis-definition/). Some newer computers in my classroom had issues with installing gensim which were resolved in [this manner](    len(https://stackoverflow.com/questions/72672196/error-pips-dependency-resolver-does-not-currently-take-into-account-all-the-pa),).\n",
    "- `pyLDAvis`: a library that is capable of visualizing your topics clusters.\n",
    "\n",
    "Topic modeling is a form of unsupervised machine learning and can be really helpful in discovering topics in a large amount of text, especially if you're uncertain which topics might be buried in thousands or millions of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced40d95-a0fd-45bc-8e3d-018afc307fe6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "# for comprehension of language\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "\n",
    "# for topics modeling\n",
    "import gensim \n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel, LsiModel, HdpModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47309a2e-25da-4775-8139-1ef3b214fe5a",
   "metadata": {},
   "source": [
    "### Load spaCy's English language trained pipeline\n",
    "\n",
    "`A training pipeline typically reads training data from a feature store, performs model-dependent transformations, trains the model, and evaluates the model before the model is saved to a model registry.`\n",
    "\n",
    "You will need to download one of spaCy's models and can do so by typing this into a cell here:\n",
    "```\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535f8d12-1144-467a-bbaa-ef2700fe5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09006585-4b45-45c9-9578-02a9abe0d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the English language model \n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18926fb2-aa30-4e55-8dcc-0e41fbed4a71",
   "metadata": {},
   "source": [
    "#### Stop words\n",
    "\n",
    "A lot of languages also contain 'stop words', words that are used very frequently and may not be useful when we're evaluating how often certain words may be used. spaCy has niftyfunctions that allow us to designate stop words for our analysis. \n",
    "\n",
    "For this purpose, we got stopwords [here](https://gist.github.com/sebleier/554280).\n",
    "\n",
    "First we need to open the text file adn then turn it into a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e9e53a-24fa-418f-b368-d494b6aa7e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430 ['a', 'about', 'above', 'across', 'after', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'an', 'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'are', 'area', 'areas', 'around', 'as', 'ask', 'asked', 'asking', 'asks', 'at', 'away', 'b', 'back', 'backed', 'backing', 'backs', 'be', 'became', 'because', 'become', 'becomes', 'been', 'before', 'began', 'behind', 'being', 'beings', 'best', 'better', 'between', 'big', 'both', 'but', 'by', 'c', 'came', 'can', 'cannot', 'case', 'cases', 'certain', 'certainly', 'clear', 'clearly', 'come', 'could', 'd', 'did', 'differ', 'different', 'differently', 'do', 'does', 'done', 'down', 'down', 'downed', 'downing', 'downs', 'during', 'e', 'each', 'early', 'either', 'end', 'ended', 'ending', 'ends', 'enough', 'even', 'evenly', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'f', 'face', 'faces', 'fact', 'facts', 'far', 'felt', 'few', 'find', 'finds', 'first', 'for', 'four', 'from', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthers', 'g', 'gave', 'general', 'generally', 'get', 'gets', 'give', 'given', 'gives', 'go', 'going', 'good', 'goods', 'got', 'great', 'greater', 'greatest', 'group', 'grouped', 'grouping', 'groups', 'h', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'herself', 'high', 'high', 'high', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'however', 'i', 'if', 'important', 'in', 'interest', 'interested', 'interesting', 'interests', 'into', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kind', 'knew', 'know', 'known', 'knows', 'l', 'large', 'largely', 'last', 'later', 'latest', 'least', 'less', 'let', 'lets', 'like', 'likely', 'long', 'longer', 'longest', 'm', 'made', 'make', 'making', 'man', 'many', 'may', 'me', 'member', 'members', 'men', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'n', 'necessary', 'need', 'needed', 'needing', 'needs', 'never', 'new', 'new', 'newer', 'newest', 'next', 'no', 'nobody', 'non', 'noone', 'not', 'nothing', 'now', 'nowhere', 'number', 'numbers', 'o', 'of', 'off', 'often', 'old', 'older', 'oldest', 'on', 'once', 'one', 'only', 'open', 'opened', 'opening', 'opens', 'or', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'our', 'out', 'over', 'p', 'part', 'parted', 'parting', 'parts', 'per', 'perhaps', 'place', 'places', 'point', 'pointed', 'pointing', 'points', 'possible', 'present', 'presented', 'presenting', 'presents', 'problem', 'problems', 'put', 'puts', 'q', 'quite', 'r', 'rather', 'really', 'right', 'right', 'room', 'rooms', 's', 'said', 'same', 'saw', 'say', 'says', 'second', 'seconds', 'see', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', 'she', 'should', 'show', 'showed', 'showing', 'shows', 'side', 'sides', 'since', 'small', 'smaller', 'smallest', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', 'state', 'states', 'still', 'still', 'such', 'sure', 't', 'take', 'taken', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'these', 'they', 'thing', 'things', 'think', 'thinks', 'this', 'those', 'though', 'thought', 'thoughts', 'three', 'through', 'thus', 'to', 'today', 'together', 'too', 'took', 'toward', 'turn', 'turned', 'turning', 'turns', 'two', 'u', 'under', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'v', 'very', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', 'way', 'ways', 'we', 'well', 'wells', 'went', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'worked', 'working', 'works', 'would', 'x', 'y', 'year', 'years', 'yet', 'you', 'young', 'younger', 'youngest', 'your', 'yours', 'z', '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"../data/stopwords.txt\", \"r\") as file:\n",
    "    stop_words = file.read().split(\"\\n\")\n",
    "\n",
    "print(\n",
    "    len(stop_words), \n",
    "    stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343e2a1-7878-445a-b7e6-5a1addb27aa2",
   "metadata": {},
   "source": [
    "Next we use spaCy's model and define stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4113aa82-5a8a-40cc-9b80-a90236dc48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# starts a loop that iterates through each word in the stop_words list.\n",
    "for stopword in stop_words:\n",
    "    # This line retrieves the lexeme (the base or dictionary form of a word) from spaCy's vocabulary. \n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    # then we set `lexeme.is_stop = True`for each word, making every word a stop word in spaCy's vocabulary.\n",
    "    lexeme.is_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbdecb-b221-4888-b06a-0ae2b2f9f4ed",
   "metadata": {},
   "source": [
    "## Loading your text and making it a corpus\n",
    "\n",
    "#### First we need to load the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b9bbc4d-422e-41ce-b170-1581aecd2273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../output/demure/Replying to @DanaLynn #fyp .mp4</td>\n",
       "      <td>Ok, vamos a tener un honesto sobre estos híga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../output/demure/Sweatproof glam #fyp #demure ...</td>\n",
       "      <td>Es sword RW Fan. Buenas noches. El Ice Swap t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../output/demure/#fyp #demure .mp4</td>\n",
       "      <td>¿Qué es lo que hace para trabajar? Muy dimulo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../output/demure/Replying to @Bri G. #fyp @Ken...</td>\n",
       "      <td>ANS FLES magia, porque yo también estaba pensa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../output/demure/#fyp.mp4</td>\n",
       "      <td>¿Dye aunque te decimos a primaria? Me pusebet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  \\\n",
       "0   ../output/demure/Replying to @DanaLynn #fyp .mp4   \n",
       "1  ../output/demure/Sweatproof glam #fyp #demure ...   \n",
       "2                 ../output/demure/#fyp #demure .mp4   \n",
       "3  ../output/demure/Replying to @Bri G. #fyp @Ken...   \n",
       "4                          ../output/demure/#fyp.mp4   \n",
       "\n",
       "                                          transcript  \n",
       "0   Ok, vamos a tener un honesto sobre estos híga...  \n",
       "1   Es sword RW Fan. Buenas noches. El Ice Swap t...  \n",
       "2   ¿Qué es lo que hace para trabajar? Muy dimulo...  \n",
       "3  ANS FLES magia, porque yo también estaba pensa...  \n",
       "4   ¿Dye aunque te decimos a primaria? Me pusebet...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load a spreadsheet with the text you want to analyze\n",
    "tiktok_influencer =  pd.read_csv(\"../output/transcripts.csv\")\n",
    "\n",
    "print(len(tiktok_influencer))\n",
    "tiktok_influencer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509699aa-b11b-425a-b9b7-846f0372ba59",
   "metadata": {},
   "source": [
    "The next lines take all content from the `transcript` column, turn it into a list and then join it all with a space between each text. This creates one large corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8065628-6859-48ac-97c7-857d9840b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_list = tiktok_influencer[\"transcript\"].tolist()\n",
    " \n",
    "text = ' '.join(str(x) for x in transcript_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e829d05-4fe3-4504-82b0-fb5c4eefc9c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2900"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62b3040-370c-4d0f-94b9-44665c4c612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bf634d9-48de-4743-9592-fa53a06d0710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff990b1-1877-49ca-b463-7d63aa5c8f2c",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "The next few lines 'normalize' the text and turns words into lemmas, get rid of stopwords and punctuation markers, and add lemmatized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbcefef3-b7a5-4ee3-ab29-79b6cb3dcd07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the lemma for the word   is  \n",
      "the lemma for the word Ok is Ok\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word vamos is vamos\n",
      "the lemma for the word a is a\n",
      "the lemma for the word tener is tener\n",
      "the lemma for the word un is un\n",
      "the lemma for the word honesto is honesto\n",
      "the lemma for the word sobre is sobre\n",
      "the lemma for the word estos is estos\n",
      "the lemma for the word hígamos is hígamos\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Yo is Yo\n",
      "the lemma for the word siempre is siempre\n",
      "the lemma for the word he is he\n",
      "the lemma for the word habido is habido\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Por is Por\n",
      "the lemma for the word 10 is 10\n",
      "the lemma for the word años is año\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word exclusivamente is exclusivamente\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word no is no\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word en is en\n",
      "the lemma for the word realidad is realidad\n",
      "the lemma for the word . is .\n",
      "the lemma for the word La is La\n",
      "the lemma for the word más is más\n",
      "the lemma for the word importante is importante\n",
      "the lemma for the word es is es\n",
      "the lemma for the word que is que\n",
      "the lemma for the word el is el\n",
      "the lemma for the word color is color\n",
      "the lemma for the word de is de\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word es is es\n",
      "the lemma for the word buitufla is buitufla\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Si is Si\n",
      "the lemma for the word usted is usted\n",
      "the lemma for the word sabe is sabe\n",
      "the lemma for the word como is como\n",
      "the lemma for the word el is el\n",
      "the lemma for the word color is color\n",
      "the lemma for the word de is de\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word o is o\n",
      "the lemma for the word el is el\n",
      "the lemma for the word color is color\n",
      "the lemma for the word de is de\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word Baby is Baby\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word es is es\n",
      "the lemma for the word la is la\n",
      "the lemma for the word forma is forma\n",
      "the lemma for the word de is de\n",
      "the lemma for the word ir is ir\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Ahora is Ahora\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word es is es\n",
      "the lemma for the word para is para\n",
      "the lemma for the word mi is mi\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Para is para\n",
      "the lemma for the word cómo is cómo\n",
      "the lemma for the word es is es\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word y is y\n",
      "the lemma for the word cómo is cómo\n",
      "the lemma for the word es is es\n",
      "the lemma for the word rica is rica\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Obviamente is Obviamente\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word ¿ is ¿\n",
      "the lemma for the word cómo is cómo\n",
      "the lemma for the word hay is hay\n",
      "the lemma for the word que is que\n",
      "the lemma for the word tener is tener\n",
      "the lemma for the word que is que\n",
      "the lemma for the word tener is tener\n",
      "the lemma for the word que is que\n",
      "the lemma for the word tener is tener\n",
      "the lemma for the word que is que\n",
      "the lemma for the word tener is tener\n",
      "the lemma for the word en is en\n",
      "the lemma for the word esa is esa\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word ? is ?\n",
      "the lemma for the word Estamos is Estamos\n",
      "the lemma for the word mirando is mirando\n",
      "the lemma for the word en is en\n",
      "the lemma for the word un is un\n",
      "the lemma for the word par is par\n",
      "the lemma for the word de is de\n",
      "the lemma for the word $ is $\n",
      "the lemma for the word 1500 is 1500\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Y is Y\n",
      "the lemma for the word luego is luego\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word en is en\n",
      "the lemma for the word el is el\n",
      "the lemma for the word coste is coste\n",
      "the lemma for the word de is de\n",
      "the lemma for the word color is color\n",
      "the lemma for the word de is de\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word como is como\n",
      "the lemma for the word los is los\n",
      "the lemma for the word materiales is materiales\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word estamos is estamos\n",
      "the lemma for the word casi is casi\n",
      "the lemma for the word mirando is mirando\n",
      "the lemma for the word en is en\n",
      "the lemma for the word $ is $\n",
      "the lemma for the word 2000 is 2000\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Y is Y\n",
      "the lemma for the word el is el\n",
      "the lemma for the word que is que\n",
      "the lemma for the word es is es\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word ya is ya\n",
      "the lemma for the word lo is lo\n",
      "the lemma for the word voy is voy\n",
      "the lemma for the word a is a\n",
      "the lemma for the word mirar is mirar\n",
      "the lemma for the word mis is mis\n",
      "the lemma for the word videos is videos\n",
      "the lemma for the word de is de\n",
      "the lemma for the word mi is mi\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word . is .\n",
      "the lemma for the word En is en\n",
      "the lemma for the word mi is mi\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word se is se\n",
      "the lemma for the word le is le\n",
      "the lemma for the word parece is parece\n",
      "the lemma for the word bien is bien\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Baby is Baby\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word ¿ is ¿\n",
      "the lemma for the word cómo is cómo\n",
      "the lemma for the word vas is vas\n",
      "the lemma for the word a is a\n",
      "the lemma for the word estar is estar\n",
      "the lemma for the word en is en\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word ? is ?\n",
      "the lemma for the word Es is es\n",
      "the lemma for the word un is un\n",
      "the lemma for the word poco is poco\n",
      "the lemma for the word de is de\n",
      "the lemma for the word smoky is smoky\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Y is y\n",
      "the lemma for the word no is no\n",
      "the lemma for the word me is I\n",
      "the lemma for the word lo is lo\n",
      "the lemma for the word voy is voy\n",
      "the lemma for the word a is a\n",
      "the lemma for the word hacer is hacer\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Me is I\n",
      "the lemma for the word voy is voy\n",
      "the lemma for the word a is a\n",
      "the lemma for the word estar is estar\n",
      "the lemma for the word en is en\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Under is under\n",
      "the lemma for the word the is the\n",
      "the lemma for the word hood is hood\n",
      "the lemma for the word . is .\n",
      "the lemma for the word ¡ is ¡\n",
      "the lemma for the word Ah is Ah\n",
      "the lemma for the word ! is !\n",
      "the lemma for the word ¡ is ¡\n",
      "the lemma for the word Ah is Ah\n",
      "the lemma for the word ! is !\n",
      "the lemma for the word La is La\n",
      "the lemma for the word última is última\n",
      "the lemma for the word cosa is cosa\n",
      "the lemma for the word que is que\n",
      "the lemma for the word me is I\n",
      "the lemma for the word ha is ha\n",
      "the lemma for the word hecho is hecho\n",
      "the lemma for the word es is es\n",
      "the lemma for the word que is que\n",
      "the lemma for the word un is un\n",
      "the lemma for the word par is par\n",
      "the lemma for the word de is de\n",
      "the lemma for the word $ is $\n",
      "the lemma for the word 1500 is 1500\n",
      "the lemma for the word se is se\n",
      "the lemma for the word me is I\n",
      "the lemma for the word va is va\n",
      "the lemma for the word a is a\n",
      "the lemma for the word ser is ser\n",
      "the lemma for the word una is una\n",
      "the lemma for the word cosa is cosa\n",
      "the lemma for the word que is que\n",
      "the lemma for the word no is no\n",
      "the lemma for the word puede is puede\n",
      "the lemma for the word ser is ser\n",
      "the lemma for the word . is .\n",
      "the lemma for the word ¡ is ¡\n",
      "the lemma for the word Ah is Ah\n",
      "the lemma for the word ! is !\n",
      "the lemma for the word ¿ is ¿\n",
      "the lemma for the word Qué is Qué\n",
      "the lemma for the word es is es\n",
      "the lemma for the word la is la\n",
      "the lemma for the word nueva is nueva\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word ? is ?\n",
      "the lemma for the word Es is es\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Es is Es\n",
      "the lemma for the word que is que\n",
      "the lemma for the word se is se\n",
      "the lemma for the word le is le\n",
      "the lemma for the word parece is parece\n",
      "the lemma for the word bien is bien\n",
      "the lemma for the word . is .\n",
      "the lemma for the word La is La\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word es is es\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Es is es\n",
      "the lemma for the word sólo is sólo\n",
      "the lemma for the word para is para\n",
      "the lemma for the word mí is mí\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Y is Y\n",
      "the lemma for the word luego is luego\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word es is es\n",
      "the lemma for the word una is una\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Es is es\n",
      "the lemma for the word como is como\n",
      "the lemma for the word mi is mi\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word es is es\n",
      "the lemma for the word una is una\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word ¡ is ¡\n",
      "the lemma for the word Ah is Ah\n",
      "the lemma for the word ! is !\n",
      "the lemma for the word Es is es\n",
      "the lemma for the word como is como\n",
      "the lemma for the word ... is ...\n",
      "the lemma for the word Yo is Yo\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word por is por\n",
      "the lemma for the word tanto is tanto\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word me is I\n",
      "the lemma for the word he is he\n",
      "the lemma for the word visto is visto\n",
      "the lemma for the word en is en\n",
      "the lemma for the word mi is mi\n",
      "the lemma for the word cabeza is cabeza\n",
      "the lemma for the word y is y\n",
      "the lemma for the word me is I\n",
      "the lemma for the word he is he\n",
      "the lemma for the word visto is visto\n",
      "the lemma for the word en is en\n",
      "the lemma for the word mi is mi\n",
      "the lemma for the word casa is casa\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Tú is tú\n",
      "the lemma for the word sabes is sabe\n",
      "the lemma for the word que is que\n",
      "the lemma for the word es is es\n",
      "the lemma for the word un is un\n",
      "the lemma for the word gran is gran\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Pero is Pero\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word me is I\n",
      "the lemma for the word gusta is gusta\n",
      "the lemma for the word mucho is mucho\n",
      "the lemma for the word el is el\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Así is Así\n",
      "the lemma for the word que is que\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word yo is yo\n",
      "the lemma for the word me is me\n",
      "the lemma for the word puedo is puedo\n",
      "the lemma for the word hacer is hacer\n",
      "the lemma for the word la is la\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Pero is Pero\n",
      "the lemma for the word me is I\n",
      "the lemma for the word voy is voy\n",
      "the lemma for the word a is a\n",
      "the lemma for the word tener is tener\n",
      "the lemma for the word un is un\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Oh is oh\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Un is Un\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word es is es\n",
      "the lemma for the word $ is $\n",
      "the lemma for the word 2000 is 2000\n",
      "the lemma for the word de is de\n",
      "the lemma for the word cintura is cintura\n",
      "the lemma for the word de is de\n",
      "the lemma for the word boteo is boteo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Me is I\n",
      "the lemma for the word voy is voy\n",
      "the lemma for the word a is a\n",
      "the lemma for the word estar is estar\n",
      "the lemma for the word en is en\n",
      "the lemma for the word el is el\n",
      "the lemma for the word internet is internet\n",
      "the lemma for the word . is .\n",
      "the lemma for the word   is  \n",
      "the lemma for the word Es is Es\n",
      "the lemma for the word sword is sword\n",
      "the lemma for the word RW is RW\n",
      "the lemma for the word Fan is Fan\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Buenas is buena\n",
      "the lemma for the word noches is noche\n",
      "the lemma for the word . is .\n",
      "the lemma for the word El is El\n",
      "the lemma for the word Ice is Ice\n",
      "the lemma for the word Swap is Swap\n",
      "the lemma for the word tiene is tiene\n",
      "the lemma for the word Among is Among\n",
      "the lemma for the word . is .\n",
      "the lemma for the word ¡ is ¡\n",
      "the lemma for the word Eh is Eh\n",
      "the lemma for the word holaen is holaen\n",
      "the lemma for the word ! is !\n",
      "the lemma for the word Que is Que\n",
      "the lemma for the word puedo is puedo\n",
      "the lemma for the word decir is decir\n",
      "the lemma for the word ven is ven\n",
      "the lemma for the word ... is ...\n",
      "the lemma for the word Esto is esto\n",
      "the lemma for the word esta is esta\n",
      "the lemma for the word tranquilo is tranquilo\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word ¡ is ¡\n",
      "the lemma for the word ah is ah\n",
      "the lemma for the word bueno is bueno\n",
      "the lemma for the word Пр is Пр\n",
      "the lemma for the word crypto is crypto\n",
      "the lemma for the word ! is !\n",
      "the lemma for the word Fuéno is Fuéno\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word fú하면 is fú하면\n",
      "the lemma for the word Shooting is Shooting\n",
      "the lemma for the word . is .\n",
      "the lemma for the word ¡ is ¡\n",
      "the lemma for the word Eh is Eh\n",
      "the lemma for the word holaen is holaen\n",
      "the lemma for the word ! is !\n",
      "the lemma for the word   is  \n",
      "the lemma for the word ¿ is ¿\n",
      "the lemma for the word Qué is Qué\n",
      "the lemma for the word es is es\n",
      "the lemma for the word lo is lo\n",
      "the lemma for the word que is que\n",
      "the lemma for the word hace is hace\n",
      "the lemma for the word para is para\n",
      "the lemma for the word trabajar is trabajar\n",
      "the lemma for the word ? is ?\n",
      "the lemma for the word Muy is Muy\n",
      "the lemma for the word dimuloso is dimuloso\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word muy is muy\n",
      "the lemma for the word mindful is mindful\n",
      "the lemma for the word . is .\n",
      "the lemma for the word No is no\n",
      "the lemma for the word me is me\n",
      "the lemma for the word gusta is gusta\n",
      "the lemma for the word trabajar is trabajar\n",
      "the lemma for the word con is con\n",
      "the lemma for the word un is un\n",
      "the lemma for the word coche is coche\n",
      "the lemma for the word con is con\n",
      "the lemma for the word un is un\n",
      "the lemma for the word coche is coche\n",
      "the lemma for the word con is con\n",
      "the lemma for the word un is un\n",
      "the lemma for the word coche is coche\n",
      "the lemma for the word . is .\n",
      "the lemma for the word No is no\n",
      "the lemma for the word me is me\n",
      "the lemma for the word parece is parece\n",
      "the lemma for the word como is como\n",
      "the lemma for the word un is un\n",
      "the lemma for the word clon is clon\n",
      "the lemma for the word cuando is cuando\n",
      "the lemma for the word me is I\n",
      "the lemma for the word voy is voy\n",
      "the lemma for the word a is a\n",
      "the lemma for the word trabajar is trabajar\n",
      "the lemma for the word . is .\n",
      "the lemma for the word No is no\n",
      "the lemma for the word me is me\n",
      "the lemma for the word gusta is gusta\n",
      "the lemma for the word mucho is mucho\n",
      "the lemma for the word mucho is mucho\n",
      "the lemma for the word a is a\n",
      "the lemma for the word lo is lo\n",
      "the lemma for the word que is que\n",
      "the lemma for the word me is I\n",
      "the lemma for the word gusta is gusta\n",
      "the lemma for the word . is .\n",
      "the lemma for the word ¿ is ¿\n",
      "the lemma for the word Qué is Qué\n",
      "the lemma for the word es is es\n",
      "the lemma for the word lo is lo\n",
      "the lemma for the word que is que\n",
      "the lemma for the word hace is hace\n",
      "the lemma for the word para is para\n",
      "the lemma for the word trabajar is trabajar\n",
      "the lemma for the word ? is ?\n",
      "the lemma for the word La is La\n",
      "the lemma for the word forma is forma\n",
      "the lemma for the word que is que\n",
      "the lemma for the word me is I\n",
      "the lemma for the word puse is puse\n",
      "the lemma for the word en is en\n",
      "the lemma for the word la is la\n",
      "the lemma for the word interview is interview\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word es is es\n",
      "the lemma for the word la is la\n",
      "the lemma for the word forma is forma\n",
      "the lemma for the word que is que\n",
      "the lemma for the word me is I\n",
      "the lemma for the word puse is puse\n",
      "the lemma for the word en is en\n",
      "the lemma for the word la is la\n",
      "the lemma for the word casa is casa\n",
      "the lemma for the word . is .\n",
      "the lemma for the word A is a\n",
      "the lemma for the word veces is vece\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word a is a\n",
      "the lemma for the word veces is vece\n",
      "the lemma for the word me is I\n",
      "the lemma for the word puse is puse\n",
      "the lemma for the word en is en\n",
      "the lemma for the word la is la\n",
      "the lemma for the word interview is interview\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word Look is look\n",
      "the lemma for the word like is like\n",
      "the lemma for the word Marge is Marge\n",
      "the lemma for the word Simpson is Simpson\n",
      "the lemma for the word y is y\n",
      "the lemma for the word la is la\n",
      "the lemma for the word casa is casa\n",
      "the lemma for the word de is de\n",
      "the lemma for the word la is la\n",
      "the lemma for the word casa is casa\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word Look is look\n",
      "the lemma for the word like is like\n",
      "the lemma for the word Patty is Patty\n",
      "the lemma for the word and is and\n",
      "the lemma for the word Selma is Selma\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word no is no\n",
      "the lemma for the word es is es\n",
      "the lemma for the word dimuloso is dimuloso\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Estoy is Estoy\n",
      "the lemma for the word muy is muy\n",
      "the lemma for the word modesto is modesto\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word estoy is estoy\n",
      "the lemma for the word muy is muy\n",
      "the lemma for the word mindful is mindful\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Una is Una\n",
      "the lemma for the word vez is vez\n",
      "the lemma for the word que is que\n",
      "the lemma for the word son is son\n",
      "the lemma for the word mi is mi\n",
      "the lemma for the word myślę is myślę\n",
      "the lemma for the word ... is ...\n",
      "the lemma for the word es is es\n",
      "the lemma for the word muy is muy\n",
      "the lemma for the word muy is muy\n",
      "the lemma for the word convince is convince\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word no is no\n",
      "the lemma for the word más is más\n",
      "the lemma for the word la is la\n",
      "the lemma for the word Korea is Korea\n",
      "the lemma for the word . is .\n",
      "the lemma for the word El is El\n",
      "the lemma for the word resolved is resolve\n",
      "the lemma for the word para is para\n",
      "the lemma for the word ver is ver\n",
      "the lemma for the word si is si\n",
      "the lemma for the word tú is tú\n",
      "the lemma for the word extenso is extenso\n",
      "the lemma for the word de is de\n",
      "the lemma for the word nada is nada\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Es is es\n",
      "the lemma for the word si is si\n",
      "the lemma for the word lo is lo\n",
      "the lemma for the word debúte is debúte\n",
      "the lemma for the word de is de\n",
      "the lemma for the word ver is ver\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word fy is fy\n",
      "the lemma for the word uno is uno\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Qué is Qué\n",
      "the lemma for the word es is es\n",
      "the lemma for the word los is los\n",
      "the lemma for the word que is que\n",
      "the lemma for the word me is I\n",
      "the lemma for the word les is les\n",
      "the lemma for the word gusta is gusta\n",
      "the lemma for the word comiendo is comiendo\n",
      "the lemma for the word . is .\n",
      "the lemma for the word ANS is ANS\n",
      "the lemma for the word FLES is FLES\n",
      "the lemma for the word magia is magia\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word porque is porque\n",
      "the lemma for the word yo is yo\n",
      "the lemma for the word también is también\n",
      "the lemma for the word estaba is estaba\n",
      "the lemma for the word pensando is pensando\n",
      "the lemma for the word a is a\n",
      "the lemma for the word unos is unos\n",
      "the lemma for the word parcono is parcono\n",
      "the lemma for the word . is .\n",
      "the lemma for the word porque is porque\n",
      "the lemma for the word yosta is yosta\n",
      "the lemma for the word el is el\n",
      "the lemma for the word excuse is excuse\n",
      "the lemma for the word . is .\n",
      "the lemma for the word   is  \n",
      "the lemma for the word ¿ is ¿\n",
      "the lemma for the word Dye is Dye\n",
      "the lemma for the word aunque is aunque\n",
      "the lemma for the word te is te\n",
      "the lemma for the word decimos is decimos\n",
      "the lemma for the word a is a\n",
      "the lemma for the word primaria is primaria\n",
      "the lemma for the word ? is ?\n",
      "the lemma for the word Me is I\n",
      "the lemma for the word pusebetween is pusebetween\n",
      "the lemma for the word con is con\n",
      "the lemma for the word mi is mi\n",
      "the lemma for the word familia is familia\n",
      "the lemma for the word . is .\n",
      "the lemma for the word N is n\n",
      "the lemma for the word conversations is conversation\n",
      "the lemma for the word   is  \n",
      "the lemma for the word sector is sector\n",
      "the lemma for the word aggressive is aggressive\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word mariamos is mariamos\n",
      "the lemma for the word de is de\n",
      "the lemma for the word haber is haber\n",
      "the lemma for the word 150 is 150\n",
      "the lemma for the word dinero is dinero\n",
      "the lemma for the word . is .\n",
      "the lemma for the word Mmm is Mmm\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word bueno is bueno\n",
      "the lemma for the word … is …\n",
      "the lemma for the word Si is Si\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word el is el\n",
      "the lemma for the word doctor is doctor\n",
      "the lemma for the word coyote is coyote\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word ¿ is ¿\n",
      "the lemma for the word t is t\n",
      "the lemma for the word försito is försito\n",
      "the lemma for the word Богお願いします is Богお願いします\n",
      "the lemma for the word ? is ?\n",
      "the lemma for the word ¡ is ¡\n",
      "the lemma for the word o is o\n",
      "the lemma for the word todo is todo\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word ay is ay\n",
      "the lemma for the word ! is !\n",
      "the lemma for the word ¡ is ¡\n",
      "the lemma for the word em is em\n",
      "the lemma for the word це is це\n",
      "the lemma for the word dizzy is dizzy\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word él is él\n",
      "the lemma for the word tiene is tiene\n",
      "the lemma for the word Tiere is Tiere\n",
      "the lemma for the word autóstico is autóstico\n",
      "the lemma for the word ! is !\n",
      "the lemma for the word Así is Así\n",
      "the lemma for the word es is es\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word no is no\n",
      "the lemma for the word los is los\n",
      "the lemma for the word no is no\n",
      "the lemma for the word loMONY is loMONY\n",
      "the lemma for the word . is .\n",
      "the lemma for the word P is p\n",
      "the lemma for the word ASI is ASI\n",
      "the lemma for the word EN is EN\n",
      "the lemma for the word NO is no\n",
      "the lemma for the word PEL is PEL\n",
      "the lemma for the word LOS is LOS\n",
      "the lemma for the word V is V\n",
      "the lemma for the word isel is isel\n",
      "the lemma for the word que is que\n",
      "the lemma for the word se is se\n",
      "the lemma for the word debía<|cy| is debía<|cy|\n",
      "the lemma for the word > is >\n",
      "the lemma for the word Kurdistica is Kurdistica\n",
      "the lemma for the word de is de\n",
      "the lemma for the word Noel is Noel\n",
      "the lemma for the word Pago is Pago\n",
      "the lemma for the word , is ,\n",
      "the lemma for the word CÓN is CÓN\n",
      "the lemma for the word undeljase is undeljase\n",
      "the lemma for the word ... is ...\n"
     ]
    }
   ],
   "source": [
    "# here's a demo of us cycling through the \n",
    "for word in doc:\n",
    "    print(f\"the lemma for the word {word} is {word.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b42250-57a4-458b-9de2-21dbaba84e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95fbb2dd-5088-4da9-8cdf-5ebaa3c1a100",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ok'] 528\n"
     ]
    }
   ],
   "source": [
    "# We add some words to the stop word list\n",
    "\n",
    "#let's create some empty arrays. \n",
    "# texts will hold all our words that we will use for our topic model\n",
    "texts = []\n",
    "# is a temporary array that we will use to store the lemma-version of a word\n",
    "article = []\n",
    "\n",
    "for word in doc:\n",
    "    if word.text != '\\n' and not word.is_stop and not word.is_punct and not word.like_num:\n",
    "        article.append(word.lemma_)\n",
    "        texts.append(article)\n",
    "        article = []\n",
    "        \n",
    "print(texts[1], len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aeffef5-79c4-4499-840e-347c46c66ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' '],\n",
       " ['Ok'],\n",
       " ['vamos'],\n",
       " ['tener'],\n",
       " ['un'],\n",
       " ['honesto'],\n",
       " ['sobre'],\n",
       " ['estos'],\n",
       " ['hígamos'],\n",
       " ['Yo'],\n",
       " ['siempre'],\n",
       " ['habido'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['Por'],\n",
       " ['año'],\n",
       " ['exclusivamente'],\n",
       " ['en'],\n",
       " ['realidad'],\n",
       " ['La'],\n",
       " ['más'],\n",
       " ['importante'],\n",
       " ['es'],\n",
       " ['que'],\n",
       " ['el'],\n",
       " ['color'],\n",
       " ['de'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['es'],\n",
       " ['buitufla'],\n",
       " ['Si'],\n",
       " ['usted'],\n",
       " ['sabe'],\n",
       " ['como'],\n",
       " ['el'],\n",
       " ['color'],\n",
       " ['de'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['el'],\n",
       " ['color'],\n",
       " ['de'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['Baby'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['es'],\n",
       " ['la'],\n",
       " ['forma'],\n",
       " ['de'],\n",
       " ['ir'],\n",
       " ['Ahora'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['es'],\n",
       " ['para'],\n",
       " ['mi'],\n",
       " ['cintura'],\n",
       " ['para'],\n",
       " ['cómo'],\n",
       " ['es'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['cómo'],\n",
       " ['es'],\n",
       " ['rica'],\n",
       " ['Obviamente'],\n",
       " ['cómo'],\n",
       " ['hay'],\n",
       " ['que'],\n",
       " ['tener'],\n",
       " ['que'],\n",
       " ['tener'],\n",
       " ['que'],\n",
       " ['tener'],\n",
       " ['que'],\n",
       " ['tener'],\n",
       " ['en'],\n",
       " ['esa'],\n",
       " ['cintura'],\n",
       " ['Estamos'],\n",
       " ['mirando'],\n",
       " ['en'],\n",
       " ['un'],\n",
       " ['par'],\n",
       " ['de'],\n",
       " ['$'],\n",
       " ['Y'],\n",
       " ['luego'],\n",
       " ['en'],\n",
       " ['el'],\n",
       " ['coste'],\n",
       " ['de'],\n",
       " ['color'],\n",
       " ['de'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['como'],\n",
       " ['los'],\n",
       " ['materiales'],\n",
       " ['estamos'],\n",
       " ['casi'],\n",
       " ['mirando'],\n",
       " ['en'],\n",
       " ['$'],\n",
       " ['Y'],\n",
       " ['el'],\n",
       " ['que'],\n",
       " ['es'],\n",
       " ['ya'],\n",
       " ['lo'],\n",
       " ['voy'],\n",
       " ['mirar'],\n",
       " ['mis'],\n",
       " ['videos'],\n",
       " ['de'],\n",
       " ['mi'],\n",
       " ['cintura'],\n",
       " ['en'],\n",
       " ['mi'],\n",
       " ['cintura'],\n",
       " ['se'],\n",
       " ['le'],\n",
       " ['parece'],\n",
       " ['bien'],\n",
       " ['Baby'],\n",
       " ['cómo'],\n",
       " ['vas'],\n",
       " ['estar'],\n",
       " ['en'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['es'],\n",
       " ['un'],\n",
       " ['poco'],\n",
       " ['de'],\n",
       " ['smoky'],\n",
       " ['y'],\n",
       " ['lo'],\n",
       " ['voy'],\n",
       " ['hacer'],\n",
       " ['voy'],\n",
       " ['estar'],\n",
       " ['en'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['hood'],\n",
       " ['Ah'],\n",
       " ['Ah'],\n",
       " ['La'],\n",
       " ['última'],\n",
       " ['cosa'],\n",
       " ['que'],\n",
       " ['ha'],\n",
       " ['hecho'],\n",
       " ['es'],\n",
       " ['que'],\n",
       " ['un'],\n",
       " ['par'],\n",
       " ['de'],\n",
       " ['$'],\n",
       " ['se'],\n",
       " ['va'],\n",
       " ['ser'],\n",
       " ['una'],\n",
       " ['cosa'],\n",
       " ['que'],\n",
       " ['puede'],\n",
       " ['ser'],\n",
       " ['Ah'],\n",
       " ['Qué'],\n",
       " ['es'],\n",
       " ['la'],\n",
       " ['nueva'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['es'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['Es'],\n",
       " ['que'],\n",
       " ['se'],\n",
       " ['le'],\n",
       " ['parece'],\n",
       " ['bien'],\n",
       " ['La'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['es'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['es'],\n",
       " ['sólo'],\n",
       " ['para'],\n",
       " ['mí'],\n",
       " ['Y'],\n",
       " ['luego'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['es'],\n",
       " ['una'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['es'],\n",
       " ['como'],\n",
       " ['mi'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['es'],\n",
       " ['una'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['Ah'],\n",
       " ['es'],\n",
       " ['como'],\n",
       " ['Yo'],\n",
       " ['por'],\n",
       " ['tanto'],\n",
       " ['visto'],\n",
       " ['en'],\n",
       " ['mi'],\n",
       " ['cabeza'],\n",
       " ['visto'],\n",
       " ['en'],\n",
       " ['mi'],\n",
       " ['casa'],\n",
       " ['tú'],\n",
       " ['sabe'],\n",
       " ['que'],\n",
       " ['es'],\n",
       " ['un'],\n",
       " ['gran'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['Pero'],\n",
       " ['gusta'],\n",
       " ['mucho'],\n",
       " ['el'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['Así'],\n",
       " ['que'],\n",
       " ['yo'],\n",
       " ['puedo'],\n",
       " ['hacer'],\n",
       " ['la'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['Pero'],\n",
       " ['voy'],\n",
       " ['tener'],\n",
       " ['un'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['oh'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['Un'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['es'],\n",
       " ['$'],\n",
       " ['de'],\n",
       " ['cintura'],\n",
       " ['de'],\n",
       " ['boteo'],\n",
       " ['voy'],\n",
       " ['estar'],\n",
       " ['en'],\n",
       " ['el'],\n",
       " ['internet'],\n",
       " [' '],\n",
       " ['Es'],\n",
       " ['sword'],\n",
       " ['RW'],\n",
       " ['Fan'],\n",
       " ['buena'],\n",
       " ['noche'],\n",
       " ['El'],\n",
       " ['Ice'],\n",
       " ['Swap'],\n",
       " ['tiene'],\n",
       " ['Eh'],\n",
       " ['holaen'],\n",
       " ['Que'],\n",
       " ['puedo'],\n",
       " ['decir'],\n",
       " ['ven'],\n",
       " ['esto'],\n",
       " ['esta'],\n",
       " ['tranquilo'],\n",
       " ['ah'],\n",
       " ['bueno'],\n",
       " ['Пр'],\n",
       " ['crypto'],\n",
       " ['Fuéno'],\n",
       " ['fú하면'],\n",
       " ['Shooting'],\n",
       " ['Eh'],\n",
       " ['holaen'],\n",
       " [' '],\n",
       " ['Qué'],\n",
       " ['es'],\n",
       " ['lo'],\n",
       " ['que'],\n",
       " ['hace'],\n",
       " ['para'],\n",
       " ['trabajar'],\n",
       " ['Muy'],\n",
       " ['dimuloso'],\n",
       " ['muy'],\n",
       " ['mindful'],\n",
       " ['gusta'],\n",
       " ['trabajar'],\n",
       " ['con'],\n",
       " ['un'],\n",
       " ['coche'],\n",
       " ['con'],\n",
       " ['un'],\n",
       " ['coche'],\n",
       " ['con'],\n",
       " ['un'],\n",
       " ['coche'],\n",
       " ['parece'],\n",
       " ['como'],\n",
       " ['un'],\n",
       " ['clon'],\n",
       " ['cuando'],\n",
       " ['voy'],\n",
       " ['trabajar'],\n",
       " ['gusta'],\n",
       " ['mucho'],\n",
       " ['mucho'],\n",
       " ['lo'],\n",
       " ['que'],\n",
       " ['gusta'],\n",
       " ['Qué'],\n",
       " ['es'],\n",
       " ['lo'],\n",
       " ['que'],\n",
       " ['hace'],\n",
       " ['para'],\n",
       " ['trabajar'],\n",
       " ['La'],\n",
       " ['forma'],\n",
       " ['que'],\n",
       " ['puse'],\n",
       " ['en'],\n",
       " ['la'],\n",
       " ['interview'],\n",
       " ['es'],\n",
       " ['la'],\n",
       " ['forma'],\n",
       " ['que'],\n",
       " ['puse'],\n",
       " ['en'],\n",
       " ['la'],\n",
       " ['casa'],\n",
       " ['vece'],\n",
       " ['vece'],\n",
       " ['puse'],\n",
       " ['en'],\n",
       " ['la'],\n",
       " ['interview'],\n",
       " ['look'],\n",
       " ['Marge'],\n",
       " ['Simpson'],\n",
       " ['la'],\n",
       " ['casa'],\n",
       " ['de'],\n",
       " ['la'],\n",
       " ['casa'],\n",
       " ['look'],\n",
       " ['Patty'],\n",
       " ['Selma'],\n",
       " ['es'],\n",
       " ['dimuloso'],\n",
       " ['Estoy'],\n",
       " ['muy'],\n",
       " ['modesto'],\n",
       " ['estoy'],\n",
       " ['muy'],\n",
       " ['mindful'],\n",
       " ['Una'],\n",
       " ['vez'],\n",
       " ['que'],\n",
       " ['son'],\n",
       " ['mi'],\n",
       " ['myślę'],\n",
       " ['es'],\n",
       " ['muy'],\n",
       " ['muy'],\n",
       " ['convince'],\n",
       " ['más'],\n",
       " ['la'],\n",
       " ['Korea'],\n",
       " ['El'],\n",
       " ['resolve'],\n",
       " ['para'],\n",
       " ['ver'],\n",
       " ['si'],\n",
       " ['tú'],\n",
       " ['extenso'],\n",
       " ['de'],\n",
       " ['nada'],\n",
       " ['es'],\n",
       " ['si'],\n",
       " ['lo'],\n",
       " ['debúte'],\n",
       " ['de'],\n",
       " ['ver'],\n",
       " ['fy'],\n",
       " ['uno'],\n",
       " ['Qué'],\n",
       " ['es'],\n",
       " ['los'],\n",
       " ['que'],\n",
       " ['les'],\n",
       " ['gusta'],\n",
       " ['comiendo'],\n",
       " ['ANS'],\n",
       " ['FLES'],\n",
       " ['magia'],\n",
       " ['porque'],\n",
       " ['yo'],\n",
       " ['también'],\n",
       " ['estaba'],\n",
       " ['pensando'],\n",
       " ['unos'],\n",
       " ['parcono'],\n",
       " ['porque'],\n",
       " ['yosta'],\n",
       " ['el'],\n",
       " ['excuse'],\n",
       " [' '],\n",
       " ['Dye'],\n",
       " ['aunque'],\n",
       " ['te'],\n",
       " ['decimos'],\n",
       " ['primaria'],\n",
       " ['pusebetween'],\n",
       " ['con'],\n",
       " ['mi'],\n",
       " ['familia'],\n",
       " ['n'],\n",
       " ['conversation'],\n",
       " [' '],\n",
       " ['sector'],\n",
       " ['aggressive'],\n",
       " ['mariamos'],\n",
       " ['de'],\n",
       " ['haber'],\n",
       " ['dinero'],\n",
       " ['Mmm'],\n",
       " ['bueno'],\n",
       " ['Si'],\n",
       " ['el'],\n",
       " ['doctor'],\n",
       " ['coyote'],\n",
       " ['försito'],\n",
       " ['Богお願いします'],\n",
       " ['todo'],\n",
       " ['ay'],\n",
       " ['em'],\n",
       " ['це'],\n",
       " ['dizzy'],\n",
       " ['él'],\n",
       " ['tiene'],\n",
       " ['Tiere'],\n",
       " ['autóstico'],\n",
       " ['Así'],\n",
       " ['es'],\n",
       " ['los'],\n",
       " ['loMONY'],\n",
       " ['p'],\n",
       " ['ASI'],\n",
       " ['EN'],\n",
       " ['PEL'],\n",
       " ['LOS'],\n",
       " ['V'],\n",
       " ['isel'],\n",
       " ['que'],\n",
       " ['se'],\n",
       " ['debía<|cy|'],\n",
       " ['>'],\n",
       " ['Kurdistica'],\n",
       " ['de'],\n",
       " ['Noel'],\n",
       " ['Pago'],\n",
       " ['CÓN'],\n",
       " ['undeljase']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffba45-d6fa-44e3-915c-deb07ac9d0fb",
   "metadata": {},
   "source": [
    "In the next lines we turn these cleaned texts into a bag-of-words format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fe4a03a-cabc-4ea1-b663-7a96dfbb53bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary() to map each unique word to a unique integer ID\n",
    "dictionary = Dictionary(texts)\n",
    "# this line creates a corpus and converts a single document (a list of words) into a bag-of-words format\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce230e8-c30b-43d8-93ef-8f68e9cda7ab",
   "metadata": {},
   "source": [
    "## Different Kinds of Topic Modeling \n",
    "Topic Modeling refers to the probabilistic modeling of text documents as topics. Gensim is one of the most popular libraries to perform such modeling.\n",
    "\n",
    "#### LSI — Latent Semantic Indexing\n",
    "One of the methods available in gensim is called LSI, which stands for Latent Semantic Indexing. LSI aims to find hidden (latent) relationships between words and concepts in a collection of documents. The assumption here is words that are used in similar contexts tend to have similar meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cbbb000-7349-4ed7-8403-3e35ac591b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '1.000*\"de\" + 0.000*\"buitufla\" + -0.000*\"oh\" + 0.000*\"te\" + -0.000*\"noche\" + 0.000*\"ya\" + 0.000*\"uno\" + -0.000*\"Ahora\" + -0.000*\"internet\" + 0.000*\"exclusivamente\"'),\n",
       " (1,\n",
       "  '-1.000*\"cintura\" + 0.000*\"honesto\" + 0.000*\"exclusivamente\" + 0.000*\"tanto\" + 0.000*\"usted\" + -0.000*\"familia\" + 0.000*\"smoky\" + 0.000*\"estoy\" + 0.000*\"FLES\" + 0.000*\"todo\"'),\n",
       " (2,\n",
       "  '1.000*\"es\" + -0.000*\"puede\" + -0.000*\"cuando\" + 0.000*\"buitufla\" + 0.000*\"LOS\" + 0.000*\"loMONY\" + 0.000*\"vas\" + -0.000*\"Que\" + -0.000*\"usted\" + 0.000*\"Kurdistica\"'),\n",
       " (3,\n",
       "  '-1.000*\"la\" + 0.000*\"casi\" + 0.000*\"va\" + 0.000*\"pusebetween\" + 0.000*\"EN\" + -0.000*\"haber\" + 0.000*\"gran\" + 0.000*\"exclusivamente\" + -0.000*\"sword\" + -0.000*\"Korea\"'),\n",
       " (4,\n",
       "  '-1.000*\"boteo\" + -0.000*\"buena\" + -0.000*\"vas\" + -0.000*\"ir\" + -0.000*\"estaba\" + -0.000*\"exclusivamente\" + 0.000*\"rica\" + -0.000*\"ASI\" + -0.000*\"Dye\" + -0.000*\"Una\"'),\n",
       " (5,\n",
       "  '-1.000*\"que\" + 0.000*\"y\" + 0.000*\"buena\" + 0.000*\"poco\" + 0.000*\"Simpson\" + -0.000*\"vamos\" + -0.000*\"habido\" + -0.000*\"debúte\" + -0.000*\"última\" + -0.000*\"Ahora\"'),\n",
       " (6,\n",
       "  '1.000*\"en\" + -0.001*\"це\" + 0.000*\"Estamos\" + 0.000*\"Que\" + -0.000*\"última\" + -0.000*\"tranquilo\" + 0.000*\"estoy\" + 0.000*\"Obviamente\" + 0.000*\"extenso\" + 0.000*\"Una\"'),\n",
       " (7,\n",
       "  '1.000*\"un\" + 0.001*\"pusebetween\" + -0.001*\"usted\" + -0.001*\"Muy\" + 0.001*\"hecho\" + 0.001*\"Estamos\" + 0.001*\"EN\" + -0.001*\"RW\" + -0.001*\"fú하면\" + 0.001*\"magia\"'),\n",
       " (8,\n",
       "  '-1.000*\"el\" + 0.001*\"poco\" + -0.001*\"ay\" + -0.001*\"ir\" + -0.001*\"exclusivamente\" + 0.001*\"modesto\" + -0.001*\"primaria\" + -0.001*\"casi\" + -0.001*\"Пр\" + -0.001*\"uno\"'),\n",
       " (9,\n",
       "  '1.000*\"mi\" + 0.002*\"esa\" + -0.002*\"försito\" + -0.002*\"Marge\" + -0.001*\"ANS\" + 0.001*\"sobre\" + -0.001*\"vas\" + 0.001*\"ha\" + 0.001*\"tranquilo\" + -0.001*\"dizzy\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)\n",
    "lsi_model.show_topics(num_topics=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231fe9a6-42b5-45cb-91c9-14f699317b78",
   "metadata": {},
   "source": [
    "#### HDP — Hierarchical Dirichlet Process\n",
    "HDP, the Hierarchical Dirichlet Process is an unsupervised Topic Model which figures out the number of topics on its own. HPD assumes that documents are mixtures of topics, and topics are mixtures of words, but it doesn't limit the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aea15908-f386-4187-a846-182ab2ec4e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.028*materiales + 0.027*nueva + 0.022*estaba + 0.020*él + 0.020*buitufla + 0.018*Swap + 0.018*uno + 0.017*sword + 0.016*Fuéno + 0.015*estar + 0.015*si + 0.014*va + 0.012*Pero + 0.012*clon + 0.011*ser + 0.011*mirar + 0.011*V + 0.010*usted + 0.010*luego + 0.010*undeljase'),\n",
       " (1,\n",
       "  '0.033*Baby + 0.023*Ok + 0.021*$ + 0.020*comiendo + 0.019*convince + 0.018*el + 0.018*V + 0.018*casi + 0.017*en + 0.015*buitufla + 0.014*försito + 0.014*muy + 0.014*hacer + 0.014*Ice + 0.013*haber + 0.013*usted + 0.013*tiene + 0.012*cuando + 0.012*voy + 0.012*trabajar'),\n",
       " (2,\n",
       "  '0.026*holaen + 0.022*es + 0.021*forma + 0.020*que + 0.018*debía<|cy| + 0.018*importante + 0.017*Así + 0.017*en + 0.016*la + 0.016*estaba + 0.015*si + 0.014*realidad + 0.013*también + 0.013*decimos + 0.012*mariamos + 0.011*familia + 0.011*ven + 0.011*smoky + 0.011*y + 0.010*resolve'),\n",
       " (3,\n",
       "  '0.032*Es + 0.026*mí + 0.025*siempre + 0.023*materiales + 0.021*trabajar + 0.021*videos + 0.018*tanto + 0.017*le + 0.016*pensando + 0.015*resolve + 0.015*habido + 0.015*la + 0.015*por + 0.013*crypto + 0.013*ASI + 0.012*Obviamente + 0.012*mindful + 0.012*importante + 0.012*Fuéno + 0.012*tranquilo'),\n",
       " (4,\n",
       "  '0.020*sobre + 0.019*Si + 0.019*para + 0.019*Una + 0.017*muy + 0.016*par + 0.015*mirar + 0.015*cosa + 0.015*dimuloso + 0.014*lo + 0.014*estar + 0.014*la + 0.013*Eh + 0.013*vez + 0.013*todo + 0.013*puedo + 0.013*bueno + 0.012*usted + 0.012*que + 0.012*>')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdp_model = HdpModel(corpus=corpus, id2word=dictionary)\n",
    "hdp_model.show_topics()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eff84d-1d3c-4a10-85a5-7b792116f635",
   "metadata": {},
   "source": [
    "#### LDA — Latent Dirichlet Allocation\n",
    "LDA or Latent Dirichlet Allocation is arguably the most famous Topic Modeling algorithm out there. Out here we create a simple Topic Model with 5 topics. The LDA algorithm assumes that each document is a mixture of topics, and each topic is a mixture of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24830a4c-8ed6-498d-affc-81be19401d41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.048*\"tener\" + 0.048*\"voy\" + 0.032*\"muy\" + 0.032*\"Qué\" + 0.032*\"se\" + 0.025*\"una\" + 0.025*\"puse\" + 0.025*\"forma\" + 0.025*\"coche\" + 0.018*\"que\"'),\n",
       " (1,\n",
       "  '0.159*\"la\" + 0.096*\"boteo\" + 0.039*\"para\" + 0.039*\"el\" + 0.027*\"es\" + 0.027*\"Ah\" + 0.020*\"un\" + 0.020*\"cómo\" + 0.014*\"$\" + 0.014*\"estar\"'),\n",
       " (2,\n",
       "  '0.163*\"de\" + 0.040*\"un\" + 0.034*\"gusta\" + 0.034*\"como\" + 0.034*\"lo\" + 0.028*\"en\" + 0.027*\"La\" + 0.021*\"el\" + 0.021*\"parece\" + 0.021*\"con\"'),\n",
       " (3,\n",
       "  '0.173*\"cintura\" + 0.143*\"es\" + 0.026*\"color\" + 0.020*\"boteo\" + 0.020*\"Y\" + 0.020*\"mucho\" + 0.014*\"trabajar\" + 0.014*\"ser\" + 0.014*\"le\" + 0.014*\"look\"'),\n",
       " (4,\n",
       "  '0.110*\"que\" + 0.086*\"de\" + 0.066*\"en\" + 0.046*\"mi\" + 0.033*\" \" + 0.027*\"casa\" + 0.021*\"boteo\" + 0.021*\"los\" + 0.014*\"puedo\" + 0.014*\"si\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(corpus=corpus, num_topics=5, id2word=dictionary)\n",
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f3d12-9624-4700-8662-a43d0305dfca",
   "metadata": {},
   "source": [
    "## Visualizing Topics with pyLDAvis\n",
    "pyLDAvis is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.\n",
    "\n",
    "The visualization is intended to be used within an IPython notebook but can also be saved to a stand-alone HTML file for easy sharing.\n",
    "\n",
    "**Note: If you have issues running the `pyLDAvis.gensim_modeles.prepare()` function, you may need to walk yourself through [these fixes](https://docs.google.com/document/d/1XOz5fJdHR754SHkMIrqCxkgD2yGHhBlueK4EcbaNwII/edit?usp=sharing).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6359801b-6e47-4fa3-9026-a1a82b590233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for visualizations\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dfc7dba-559b-4ced-9889-204b5df135da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9e9a870-2e09-45cb-8af5-cd4029c36d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, \"../output/topics_modeling_demure.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "697e38eb-c485-4434-83e3-1102152c24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5fc1b-6e06-4ffc-b832-e1f045aedc18",
   "metadata": {},
   "source": [
    "## Word count\n",
    "For good measure, we can also use this space to make a word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da9a2a3b-742c-40af-85ff-fe4f0133a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vamos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>un</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word\n",
       "0       \n",
       "1     Ok\n",
       "2  vamos\n",
       "3  tener\n",
       "4     un"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_influencer = pd.DataFrame(texts)\n",
    "print(len(words_influencer))\n",
    "words_influencer.columns = [\"word\"]\n",
    "words_influencer.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19873bb5-106f-4347-9f3b-8e79c4fde9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "de         39\n",
       "cintura    31\n",
       "es         27\n",
       "la         25\n",
       "boteo      21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tally = words_influencer[\"word\"].value_counts()\n",
    "word_tally.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b27d82c3-8481-4e42-a0c6-4d1ac113d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tally.to_csv(\"../output/word_tally_demure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f02bd-e91c-4ac8-ac0f-b2be718adca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f088c00-fbde-45b5-a621-4a331db314a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
